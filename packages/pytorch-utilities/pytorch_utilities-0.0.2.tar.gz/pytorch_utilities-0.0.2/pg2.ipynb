{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0, reduction: str = 'mean'):\n",
    "        \"\"\"\n",
    "        Initializes the DiceLoss module.\n",
    "\n",
    "        Parameters:\n",
    "            smooth (float): A smoothing factor to avoid division by zero errors.\n",
    "                Defaults to 1.0.\n",
    "            reduction (str): Specifies the reduction to apply to the output: 'none', 'mean', or 'sum':\n",
    "                'none': No reduction will be applied.\n",
    "                'mean': The mean of the losses will be returned.\n",
    "                'sum': The sum of the losses will be returned.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Calculates the Dice Loss between the inputs and targets.\n",
    "\n",
    "        Parameters:\n",
    "            inputs (torch.Tensor): The predicted segmentation map. Assumes values\n",
    "                are in the range [0, 1].\n",
    "            targets (torch.Tensor): The ground truth segmentation map. Should be a\n",
    "                binary tensor (values 0 or 1).\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The calculated Dice Loss.\n",
    "        \"\"\"\n",
    "\n",
    "        # Ensure the inputs and targets are of the same shape.\n",
    "        assert inputs.size() == targets.size(), \"Input and target must have the same size.\"\n",
    "\n",
    "        # Flatten the tensors to simplify the computation.\n",
    "        inputs_flat = inputs.view(-1)\n",
    "        targets_flat = targets.view(-1)\n",
    "\n",
    "        # Calculate the intersection and the union.\n",
    "        intersection = (inputs_flat * targets_flat).sum()\n",
    "        union = inputs_flat.sum() + targets_flat.sum()\n",
    "        \n",
    "        # Compute the Dice coefficient.\n",
    "        dice_coeff = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        # Calculate Dice loss.\n",
    "        dice_loss = 1 - dice_coeff\n",
    "\n",
    "        return dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, smooth=1.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Dice Loss for comparing the similarity of two batch of data, usually used in image segmentation.\n",
    "        \n",
    "        Parameters:\n",
    "            smooth (float, optional): A smoothing term to avoid division by zero. Defaults to 1.0.\n",
    "            reduction (str, optional): Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.\n",
    "                                       'none': no reduction will be applied.\n",
    "                                       'mean': the sum of the output will be divided by the number of elements in the output.\n",
    "                                       'sum': the output will be summed. Defaults to 'mean'.\n",
    "        \"\"\"\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        Forward pass of the Dice loss.\n",
    "        \n",
    "        Parameters:\n",
    "            input (Tensor): Predicted probabilities for each class. Shape (N, C, H, W).\n",
    "            target (Tensor): Ground truth. Shape (N, C, H, W).\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: The calculated Dice Loss.\n",
    "        \"\"\"\n",
    "        assert input.size() == target.size(), \"Input sizes must be equal.\"\n",
    "        assert input.dim() == 4, \"Input must be a 4D Tensor.\"\n",
    "        \n",
    "        input = torch.sigmoid(input)  # Apply sigmoid to clamp predictions to [0,1]\n",
    "        \n",
    "        # Flatten label and prediction tensors\n",
    "        input = input.view(-1)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        intersection = (input * target).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (input.sum() + target.sum() + self.smooth)\n",
    "        \n",
    "        if self.reduction == 'none':\n",
    "            return 1 - dice\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(1 - dice)\n",
    "        elif self.reduction == 'mean':\n",
    "            return torch.mean(1 - dice)\n",
    "        else:\n",
    "            raise ValueError(\"Reduction parameter must be 'none', 'mean', or 'sum'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai import losses\n",
    "\n",
    "\n",
    "oof = DiceLoss()\n",
    "foo = losses.DiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(32, 1, 128, 128)\n",
    "b = torch.randn(32, 1, 128, 128)\n",
    "\n",
    "loss = oof(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5001)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = torch.randint(0, 2, (32, 1, 1024, 1024))\n",
    "b = torch.randint(0, 2, (32, 1, 1024, 1024))\n",
    "\n",
    "foo(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "class CosineAnnealingLinearWarmup(_LRScheduler):\n",
    "    \"\"\"\n",
    "    Implements a warmup cosine annealing learning rate scheduler.\n",
    "\n",
    "    Attributes:\n",
    "        warmup_epochs (int): The number of warmup epochs.\n",
    "        max_epochs (int): The total number of epochs.\n",
    "        initial_lr (float): The initial learning rate.\n",
    "        cosine_annealing_epochs (int): The number of epochs for the cosine annealing phase.\n",
    "\n",
    "    Methods:\n",
    "        get_lr(): Get the learning rate for each parameter group.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: Optimizer,\n",
    "        warmup_epochs: int,\n",
    "        max_epochs: int,\n",
    "        initial_lr: float = 1e-6,\n",
    "        last_epoch: int = -1,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize a new WarmupCosineAnnealingLR instance.\n",
    "\n",
    "        Args:\n",
    "            optimizer (torch.optim.Optimizer): The optimizer for which to schedule the learning rate.\n",
    "            warmup_epochs (int): The number of warmup epochs.\n",
    "            max_epochs (int): The total number of epochs.\n",
    "            initial_lr (float, optional): The initial learning rate. Default is 1e-6.\n",
    "            last_epoch (int, optional): The index of the last epoch. Default is -1.\n",
    "        \"\"\"\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_epochs = max_epochs\n",
    "        self.initial_lr = initial_lr\n",
    "        self.cosine_annealing_epochs = self.max_epochs - self.warmup_epochs\n",
    "\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Get the learning rate for each parameter group.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: The learning rate for each parameter group.\n",
    "        \"\"\"\n",
    "        if self.last_epoch < self.warmup_epochs:\n",
    "            lr = [self.initial_lr + (base_lr - self.initial_lr) * (self.last_epoch) / self.warmup_epochs for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            cos_input = math.pi * (self.last_epoch - self.warmup_epochs) / self.cosine_annealing_epochs\n",
    "            lr = [base_lr * (1 + math.cos(cos_input)) / 2 for base_lr in self.base_lrs]\n",
    "\n",
    "        # Applying lr_scale if it exists in the parameter group (in case of layer-wise learning rate decay usage)\n",
    "        for i, param_group in enumerate(self.optimizer.param_groups):\n",
    "            if \"lr_scale\" in param_group:\n",
    "                lr[i] *= param_group[\"lr_scale\"]\n",
    "\n",
    "        return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "optim.AdamW(weight_decay=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Parameter Group 1, Current Learning Rate: 0.020000800000000003\n",
      "Epoch 2, Parameter Group 1, Current Learning Rate: 0.040000600000000004\n",
      "Epoch 3, Parameter Group 1, Current Learning Rate: 0.0600004\n",
      "Epoch 4, Parameter Group 1, Current Learning Rate: 0.08000020000000001\n",
      "Epoch 5, Parameter Group 1, Current Learning Rate: 0.1\n",
      "Epoch 6, Parameter Group 1, Current Learning Rate: 0.09997266286704631\n",
      "Epoch 7, Parameter Group 1, Current Learning Rate: 0.09989068136093873\n",
      "Epoch 8, Parameter Group 1, Current Learning Rate: 0.09975414512725057\n",
      "Epoch 9, Parameter Group 1, Current Learning Rate: 0.09956320346634877\n",
      "Epoch 10, Parameter Group 1, Current Learning Rate: 0.09931806517013612\n",
      "Epoch 11, Parameter Group 1, Current Learning Rate: 0.09901899829374047\n",
      "Epoch 12, Parameter Group 1, Current Learning Rate: 0.0986663298624003\n",
      "Epoch 13, Parameter Group 1, Current Learning Rate: 0.09826044551386744\n",
      "Epoch 14, Parameter Group 1, Current Learning Rate: 0.09780178907671788\n",
      "Epoch 15, Parameter Group 1, Current Learning Rate: 0.09729086208503174\n",
      "Epoch 16, Parameter Group 1, Current Learning Rate: 0.09672822322997304\n",
      "Epoch 17, Parameter Group 1, Current Learning Rate: 0.09611448774886924\n",
      "Epoch 18, Parameter Group 1, Current Learning Rate: 0.09545032675245813\n",
      "Epoch 19, Parameter Group 1, Current Learning Rate: 0.09473646649103817\n",
      "Epoch 20, Parameter Group 1, Current Learning Rate: 0.09397368756032445\n",
      "Epoch 21, Parameter Group 1, Current Learning Rate: 0.09316282404787869\n",
      "Epoch 22, Parameter Group 1, Current Learning Rate: 0.09230476262104677\n",
      "Epoch 23, Parameter Group 1, Current Learning Rate: 0.09140044155740101\n",
      "Epoch 24, Parameter Group 1, Current Learning Rate: 0.09045084971874738\n",
      "Epoch 25, Parameter Group 1, Current Learning Rate: 0.08945702546981969\n",
      "Epoch 26, Parameter Group 1, Current Learning Rate: 0.08842005554284296\n",
      "Epoch 27, Parameter Group 1, Current Learning Rate: 0.0873410738492077\n",
      "Epoch 28, Parameter Group 1, Current Learning Rate: 0.08622126023955445\n",
      "Epoch 29, Parameter Group 1, Current Learning Rate: 0.08506183921362444\n",
      "Epoch 30, Parameter Group 1, Current Learning Rate: 0.08386407858128707\n",
      "Epoch 31, Parameter Group 1, Current Learning Rate: 0.08262928807620844\n",
      "Epoch 32, Parameter Group 1, Current Learning Rate: 0.08135881792367686\n",
      "Epoch 33, Parameter Group 1, Current Learning Rate: 0.08005405736415126\n",
      "Epoch 34, Parameter Group 1, Current Learning Rate: 0.07871643313414718\n",
      "Epoch 35, Parameter Group 1, Current Learning Rate: 0.07734740790612135\n",
      "Epoch 36, Parameter Group 1, Current Learning Rate: 0.07594847868906077\n",
      "Epoch 37, Parameter Group 1, Current Learning Rate: 0.0745211751915254\n",
      "Epoch 38, Parameter Group 1, Current Learning Rate: 0.0730670581489344\n",
      "Epoch 39, Parameter Group 1, Current Learning Rate: 0.07158771761692465\n",
      "Epoch 40, Parameter Group 1, Current Learning Rate: 0.07008477123264849\n",
      "Epoch 41, Parameter Group 1, Current Learning Rate: 0.06855986244591104\n",
      "Epoch 42, Parameter Group 1, Current Learning Rate: 0.06701465872208215\n",
      "Epoch 43, Parameter Group 1, Current Learning Rate: 0.06545084971874737\n",
      "Epoch 44, Parameter Group 1, Current Learning Rate: 0.06387014543809225\n",
      "Epoch 45, Parameter Group 1, Current Learning Rate: 0.062274274357039965\n",
      "Epoch 46, Parameter Group 1, Current Learning Rate: 0.06066498153718736\n",
      "Epoch 47, Parameter Group 1, Current Learning Rate: 0.0590440267166055\n",
      "Epoch 48, Parameter Group 1, Current Learning Rate: 0.0574131823855921\n",
      "Epoch 49, Parameter Group 1, Current Learning Rate: 0.05577423184847932\n",
      "Epoch 50, Parameter Group 1, Current Learning Rate: 0.05412896727361663\n",
      "Epoch 51, Parameter Group 1, Current Learning Rate: 0.05247918773366113\n",
      "Epoch 52, Parameter Group 1, Current Learning Rate: 0.050826697238317925\n",
      "Epoch 53, Parameter Group 1, Current Learning Rate: 0.04917330276168208\n",
      "Epoch 54, Parameter Group 1, Current Learning Rate: 0.04752081226633889\n",
      "Epoch 55, Parameter Group 1, Current Learning Rate: 0.04587103272638339\n",
      "Epoch 56, Parameter Group 1, Current Learning Rate: 0.04422576815152071\n",
      "Epoch 57, Parameter Group 1, Current Learning Rate: 0.0425868176144079\n",
      "Epoch 58, Parameter Group 1, Current Learning Rate: 0.04095597328339451\n",
      "Epoch 59, Parameter Group 1, Current Learning Rate: 0.03933501846281267\n",
      "Epoch 60, Parameter Group 1, Current Learning Rate: 0.03772572564296005\n",
      "Epoch 61, Parameter Group 1, Current Learning Rate: 0.03612985456190778\n",
      "Epoch 62, Parameter Group 1, Current Learning Rate: 0.03454915028125265\n",
      "Epoch 63, Parameter Group 1, Current Learning Rate: 0.03298534127791785\n",
      "Epoch 64, Parameter Group 1, Current Learning Rate: 0.03144013755408897\n",
      "Epoch 65, Parameter Group 1, Current Learning Rate: 0.02991522876735154\n",
      "Epoch 66, Parameter Group 1, Current Learning Rate: 0.02841228238307536\n",
      "Epoch 67, Parameter Group 1, Current Learning Rate: 0.02693294185106562\n",
      "Epoch 68, Parameter Group 1, Current Learning Rate: 0.025478824808474593\n",
      "Epoch 69, Parameter Group 1, Current Learning Rate: 0.02405152131093924\n",
      "Epoch 70, Parameter Group 1, Current Learning Rate: 0.022652592093878668\n",
      "Epoch 71, Parameter Group 1, Current Learning Rate: 0.021283566865852824\n",
      "Epoch 72, Parameter Group 1, Current Learning Rate: 0.019945942635848746\n",
      "Epoch 73, Parameter Group 1, Current Learning Rate: 0.01864118207632315\n",
      "Epoch 74, Parameter Group 1, Current Learning Rate: 0.017370711923791565\n",
      "Epoch 75, Parameter Group 1, Current Learning Rate: 0.016135921418712958\n",
      "Epoch 76, Parameter Group 1, Current Learning Rate: 0.014938160786375571\n",
      "Epoch 77, Parameter Group 1, Current Learning Rate: 0.013778739760445552\n",
      "Epoch 78, Parameter Group 1, Current Learning Rate: 0.012658926150792322\n",
      "Epoch 79, Parameter Group 1, Current Learning Rate: 0.011579944457157043\n",
      "Epoch 80, Parameter Group 1, Current Learning Rate: 0.010542974530180327\n",
      "Epoch 81, Parameter Group 1, Current Learning Rate: 0.009549150281252633\n",
      "Epoch 82, Parameter Group 1, Current Learning Rate: 0.008599558442598998\n",
      "Epoch 83, Parameter Group 1, Current Learning Rate: 0.007695237378953235\n",
      "Epoch 84, Parameter Group 1, Current Learning Rate: 0.006837175952121305\n",
      "Epoch 85, Parameter Group 1, Current Learning Rate: 0.006026312439675552\n",
      "Epoch 86, Parameter Group 1, Current Learning Rate: 0.005263533508961827\n",
      "Epoch 87, Parameter Group 1, Current Learning Rate: 0.004549673247541886\n",
      "Epoch 88, Parameter Group 1, Current Learning Rate: 0.0038855122511307627\n",
      "Epoch 89, Parameter Group 1, Current Learning Rate: 0.003271776770026952\n",
      "Epoch 90, Parameter Group 1, Current Learning Rate: 0.002709137914968268\n",
      "Epoch 91, Parameter Group 1, Current Learning Rate: 0.0021982109232821178\n",
      "Epoch 92, Parameter Group 1, Current Learning Rate: 0.0017395544861325718\n",
      "Epoch 93, Parameter Group 1, Current Learning Rate: 0.001333670137599713\n",
      "Epoch 94, Parameter Group 1, Current Learning Rate: 0.0009810017062595322\n",
      "Epoch 95, Parameter Group 1, Current Learning Rate: 0.0006819348298638839\n",
      "Epoch 96, Parameter Group 1, Current Learning Rate: 0.0004367965336512403\n",
      "Epoch 97, Parameter Group 1, Current Learning Rate: 0.0002458548727494292\n",
      "Epoch 98, Parameter Group 1, Current Learning Rate: 0.00010931863906127326\n",
      "Epoch 99, Parameter Group 1, Current Learning Rate: 2.7337132953697554e-05\n",
      "Epoch 100, Parameter Group 1, Current Learning Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Assuming CosineAnnealingLinearWarmup is defined as per your implementation\n",
    "\n",
    "# Define a simple model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc = nn.Linear(10, 2)  # An example layer\n",
    "        self.fc2 = nn.Linear(2, 100)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x) + self.fc2(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNet()\n",
    "\n",
    "# Define an optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Initial lr doesn't matter, scheduler sets it\n",
    "optim.Adam\n",
    "\n",
    "# Parameters for the scheduler\n",
    "warmup_epochs = 5\n",
    "max_epochs = 100\n",
    "initial_lr = 1e-6\n",
    "\n",
    "# Add lr_scale to some parameter groups (optional, for demonstration)\n",
    "for i, param_group in enumerate(optimizer.param_groups):\n",
    "    param_group[\"lr_scale\"] = 1.0 #* (0.5 + i)  # Assuming a uniform scale here; adjust as needed\n",
    "\n",
    "# Instantiate the scheduler\n",
    "scheduler = CosineAnnealingLinearWarmup(optimizer, warmup_epochs, max_epochs, initial_lr)\n",
    "\n",
    "# Example training loop\n",
    "for epoch in range(max_epochs):\n",
    "    # Training step would go here\n",
    "    # ...\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "   # Print the learning rate for each parameter group\n",
    "    for i, lr in enumerate(scheduler.get_lr(), 1):\n",
    "        print(f\"Epoch {epoch+1}, Parameter Group {i}, Current Learning Rate: {lr}\")\n",
    "\n",
    "# Remember, in a real scenario, you would include loss.backward() and optimizer.step() calls to actually train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "\n",
        "storage_account_name = \"pedataplatformdev\"\n",
        "bronze_path = \"manual/CsvFromInputLoan/Rent.csv\"\n",
        "\n",
        "file_extension = 'csv'\n",
        "\n",
        "dataset_type = 'dim'\n",
        "dataset_name = 'rent'\n",
        "\n",
        "business_key_column_name = 'Quarter'\n",
        "\n",
        "sheet_name = 'LÃ¥neorganisering'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "full_silver_path = \"\"\n",
        "\n",
        "full_bronze_path = f\"abfss://bronze@{storage_account_name}.dfs.core.windows.net/{bronze_path}\"\n",
        "\n",
        "if dataset_type == \"dim\":\n",
        "    full_silver_path = f\"abfss://silver@{storage_account_name}.dfs.core.windows.net/Dimensions/Delta_executed/{dataset_name.capitalize()}/\"\n",
        "\n",
        "elif dataset_type == \"fact\":\n",
        "    full_silver_path = f\"abfss://silver@{storage_account_name}.dfs.core.windows.net/Facts/Delta_executed/{dataset_name.capitalize()}/\"\n",
        "\n",
        "else:\n",
        "    print(\"Dataset type is wrong.\")\n",
        "\n",
        "print(full_bronze_path)\n",
        "print(full_silver_path)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import explode, json_tuple\n",
        "import pandas as pd\n",
        "\n",
        "if file_extension == 'csv':\n",
        "    bronze_df = spark.read.option(\"header\", \"true\").csv(full_bronze_path)\n",
        "    #remove whitespace in column names\n",
        "    bronze_df = bronze_df.toDF(*[col.replace(\" \", \"\") for col in bronze_df.columns])\n",
        "    display(bronze_df)\n",
        "elif file_extension == 'parquet':\n",
        "    bronze_df = spark.read.parquet(full_bronze_path)\n",
        "elif file_extension == 'json': #assumes data to be flattened is located in column \"data\"\n",
        "    bronze_df = spark.read.json(full_bronze_path)\n",
        "    flattened_df = bronze_df.select(explode(bronze_df['data']).alias('exploded_column'))\n",
        "    display(flattened_df)\n",
        "elif file_extension == 'xlsx':\n",
        "    spark = SparkSession.builder.appName(\"ReadExcel\").getOrCreate()\n",
        "    df_pandas = pd.read_excel(full_bronze_path, sheet_name=sheet_name)\n",
        "    bronze_df = spark.createDataFrame(df_pandas)\n",
        "    bronze_df.show()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#sjekke om delta tabellen har data\n",
        "def exists(path):\n",
        "    try:\n",
        "        mssparkutils.fs.ls(path)\n",
        "        return True\n",
        "    except:\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.functions import col, coalesce\n",
        "\n",
        "def create_select_expression(dataset_key_name):\n",
        "    column_names = []\n",
        "    for col in bronze_df.dtypes:\n",
        "        column_names.append(col[0])\n",
        "\n",
        "    select_exprs = []\n",
        "    for column in column_names:\n",
        "        select_exprs.append(coalesce(f\"bronze_{column}\", f\"silver_{column}\").alias(column))\n",
        "\n",
        "    select_exprs.append(dataset_key_name)\n",
        "    return select_exprs\n",
        "\n",
        "#print(create_select_expression('silver_pk_dim_' + dataset_name + '_key'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col\n",
        "import sys\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "\n",
        "dataset_key_name = 'silver_pk_' + dataset_type + '_' + dataset_name + '_key'\n",
        "\n",
        "if(exists(f\"{full_silver_path}_delta_log\")):\n",
        "    print(\"Dataset already exists, executing delta load\")\n",
        "    silver_dataset_df = spark.read.format('delta').load(full_silver_path)\n",
        "\n",
        "    bronze_df_alias_prefix = bronze_df.select([F.col(c).alias(\"bronze_\"+c) for c in bronze_df.columns])\n",
        "    silver_df_alias_prefix = silver_dataset_df.select([F.col(c).alias(\"silver_\"+c) for c in silver_dataset_df.columns])\n",
        "\n",
        "    joined = bronze_df_alias_prefix.join(silver_df_alias_prefix, bronze_df_alias_prefix['bronze_' + business_key_column_name] == silver_df_alias_prefix['silver_' + business_key_column_name], how='full_outer')\n",
        "\n",
        "    select_expression = create_select_expression(dataset_key_name)\n",
        "    merged = joined.select(select_expression)\n",
        "\n",
        "    max_key = merged.selectExpr(f\"max({dataset_key_name}) as key\").collect()[0].key\n",
        "\n",
        "    new_rows_without_key = merged.filter(merged[dataset_key_name].isNull())\n",
        "\n",
        "    new_rows_array = []\n",
        "    for row in new_rows_without_key.collect():\n",
        "        max_key = max_key + 1\n",
        "        row_dict = row.asDict()\n",
        "        row_dict[dataset_key_name] = max_key\n",
        "        \n",
        "        new_rows_array.append(tuple(row_dict.values()))\n",
        "    \n",
        "    new_rows_with_generated_key = spark.createDataFrame(new_rows_array, merged.schema)\n",
        "\n",
        "    #combine new rows with existing\n",
        "    upserted_rows = merged.filter(merged[dataset_key_name].isNotNull()).union(new_rows_with_generated_key)\n",
        "\n",
        "    upserted_rows_without_prefix_column_names = upserted_rows.withColumnRenamed(dataset_key_name, dataset_key_name.replace('silver_', ''))\n",
        "\n",
        "    fixedSchema = spark.createDataFrame(upserted_rows_without_prefix_column_names.collect(), schema=silver_dataset_df.schema)\n",
        "\n",
        "\n",
        "    #TODO: Validate unique business and dataset keys\n",
        "    fixedSchema.write.format('delta').mode(\"overwrite\").save(full_silver_path)\n",
        "\n",
        "    #maybe do som validation\n",
        "else:\n",
        "    print(\"Dataset does not exists in silver, create it for the first time\")\n",
        "    #create unique ids for all rows in bronze_df\n",
        "    window_spec = Window.orderBy(business_key_column_name)\n",
        "    df_with_consecutive_id = bronze_df.withColumn(dataset_key_name.replace('silver_', ''), row_number().over(window_spec) - 1)\n",
        "\n",
        "    #write to silver\n",
        "    df_with_consecutive_id.write.format('delta').mode(\"append\").option(\"overwriteSchema\", \"true\").save(full_silver_path)"
      ]
    }
  ]
}
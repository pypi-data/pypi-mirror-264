[metadata]
name = evoeval
description = "EvoEval: Evolving Coding Benchmarks via LLM"
long_description = file: README.md
long_description_content_type = text/markdown
url = https://github.com/evo-eval/evoeval
license = Apache-2.0
license_file = LICENSE
platform = any
classifiers = 
	Operating System :: OS Independent
	Programming Language :: Python :: 3
	License :: OSI Approved :: Apache Software License

[options]
packages = find:
python_requires = >=3.9
dependency_links = 
install_requires = 
	wget>=3.2
	tempdir>=0.7.1
	multipledispatch>=0.6.0
	appdirs>=1.4.4
	numpy>=1.19.5
	tqdm>=4.56.0
	termcolor>=2.0.0
	evalplus>=0.2.0

[options.entry_points]
console_scripts = 
	evoeval.evaluate = evoeval.evaluate:main

[egg_info]
tag_build = 
tag_date = 0


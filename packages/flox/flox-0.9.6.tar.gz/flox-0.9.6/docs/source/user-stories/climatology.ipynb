{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e9bf3f9-0952-493c-a8df-4a1d851c37a9",
   "metadata": {},
   "source": [
    "# Strategies for climatology calculations\n",
    "\n",
    "This notebook is motivated by\n",
    "[this post](https://discourse.pangeo.io/t/understanding-optimal-zarr-chunking-scheme-for-a-climatology/2335)\n",
    "on the Pangeo discourse forum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac0588-ff00-43cc-b952-7ab775b24e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.array\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import flox\n",
    "import flox.xarray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f46621-1b6c-4a14-ac0f-3aa5121dad54",
   "metadata": {},
   "source": [
    "Let's first create an example Xarray Dataset representing the OISST dataset,\n",
    "with chunk sizes matching that in the post.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a91d2e2-bd6d-4b35-8002-5fac76c4c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "oisst = xr.DataArray(\n",
    "    dask.array.ones((14532, 720, 1440), chunks=(20, -1, -1)),\n",
    "    dims=(\"time\", \"lat\", \"lon\"),\n",
    "    coords={\"time\": pd.date_range(\"1981-09-01 12:00\", \"2021-06-14 12:00\", freq=\"D\")},\n",
    "    name=\"sst\",\n",
    ")\n",
    "oisst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f519ee-e575-492c-a70b-8dad63a8c222",
   "metadata": {},
   "source": [
    "To account for Feb-29 being present in some years, we'll construct a time vector to group by as \"mmm-dd\" string.\n",
    "\n",
    "For more options, see https://strftime.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c42a618-47bc-4c83-a902-ec4cf3420180",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = oisst.time.dt.strftime(\"%h-%d\").rename(\"day\")\n",
    "day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d913e7f-25bd-43c4-98b6-93bcb420c524",
   "metadata": {},
   "source": [
    "## map-reduce\n",
    "\n",
    "The default\n",
    "[method=\"map-reduce\"](https://flox.readthedocs.io/en/latest/implementation.html#method-map-reduce)\n",
    "doesn't work so well. We aggregate all days in a single ~3GB chunk.\n",
    "\n",
    "For this to work well, we'd want smaller chunks in space and bigger chunks in\n",
    "time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a14de-7526-40e3-8a97-28e84d6d6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "flox.xarray.xarray_reduce(\n",
    "    oisst,\n",
    "    day,\n",
    "    func=\"mean\",\n",
    "    method=\"map-reduce\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ad701-ea45-4555-9550-ec9daecfbea3",
   "metadata": {},
   "source": [
    "## Rechunking for map-reduce\n",
    "\n",
    "We can split each chunk along the `lat`, `lon` dimensions to make sure the\n",
    "output chunk sizes are more reasonable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c7776-9a21-4115-8ac9-9c7c6c6e2c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "flox.xarray.xarray_reduce(\n",
    "    oisst.chunk({\"lat\": -1, \"lon\": 120}),\n",
    "    day,\n",
    "    func=\"mean\",\n",
    "    method=\"map-reduce\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833f72eb-1501-4362-ae55-ec419c9f0ac1",
   "metadata": {},
   "source": [
    "But what if we didn't want to rechunk the dataset so drastically (note the 10x\n",
    "increase in tasks). For that let's try `method=\"cohorts\"`\n",
    "\n",
    "## method=cohorts\n",
    "\n",
    "We can take advantage of patterns in the groups here \"day of year\".\n",
    "Specifically:\n",
    "\n",
    "1. The groups at an approximately periodic interval, 365 or 366 days\n",
    "2. The chunk size 20 is smaller than the period of 365 or 366. This means, that\n",
    "   to construct the mean for days 1-20, we just need to use the chunks that\n",
    "   contain days 1-20.\n",
    "\n",
    "This strategy is implemented as\n",
    "[method=\"cohorts\"](https://flox.readthedocs.io/en/latest/implementation.html#method-cohorts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bafc32-7e13-41b8-90eb-b27955393392",
   "metadata": {},
   "outputs": [],
   "source": [
    "flox.xarray.xarray_reduce(\n",
    "    oisst,\n",
    "    day,\n",
    "    func=\"mean\",\n",
    "    method=\"cohorts\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e1ba0b-20e5-466a-9199-38b47029a0ed",
   "metadata": {},
   "source": [
    "By default cohorts doesn't work so well for this problem because the period\n",
    "isn't regular (365 vs 366) and the period isn't divisible by the chunk size. So\n",
    "the groups end up being \"out of phase\" (for a visual illustration\n",
    "[click here](https://flox.readthedocs.io/en/latest/implementation.html#method-cohorts)).\n",
    "Now we have the opposite problem: the chunk sizes on the output are too small.\n",
    "\n",
    "Let us inspect the cohorts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce5531-0d6c-4c89-bc44-dc2c24fa4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer codes for each \"day\"\n",
    "codes, _ = pd.factorize(day.data)\n",
    "preferred_method, cohorts = flox.core.find_group_cohorts(\n",
    "    labels=codes,\n",
    "    chunks=(oisst.chunksizes[\"time\"],),\n",
    ")\n",
    "print(len(cohorts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b4109-b7f4-4c16-918d-9a18ff2ed183",
   "metadata": {},
   "source": [
    "Looking more closely, we can see many cohorts with a single entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57983cd0-a2e0-4d16-abe6-9572f6f252bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbdbb3b-2aed-4f3f-ad20-efabb52b5e68",
   "metadata": {},
   "source": [
    "## Rechunking data for cohorts\n",
    "\n",
    "Can we fix the \"out of phase\" problem by rechunking along time?\n",
    "\n",
    "First lets see where the current chunk boundaries are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d393a5-7a4e-4d33-997b-4c422a0b8100",
   "metadata": {},
   "outputs": [],
   "source": [
    "oisst.chunksizes[\"time\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0033a3-d211-4aef-a284-c9fd3f75f6e4",
   "metadata": {},
   "source": [
    "We'll choose to rechunk such that a single month in is a chunk. This is not too different from the current chunking but will help your periodicity problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914a350-a7db-49b3-9504-6d63ff874f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newchunks = xr.ones_like(day).astype(int).resample(time=\"M\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a884bc-1b71-4874-8143-73b3b5c41458",
   "metadata": {},
   "outputs": [],
   "source": [
    "rechunked = oisst.chunk(time=tuple(newchunks.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7a27f-ebab-4673-bb9f-80620389994b",
   "metadata": {},
   "source": [
    "And now our cohorts contain more than one group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522fb82-764d-4e4e-8337-a5123e3088f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preferrd_method, new_cohorts = flox.core.find_group_cohorts(\n",
    "    labels=codes,\n",
    "    chunks=(rechunked.chunksizes[\"time\"],),\n",
    ")\n",
    "# one cohort per month!\n",
    "len(new_cohorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b6f70-c057-4783-ad55-21b20ff27e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cohorts.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ac39c-dd84-4375-a884-0c1c3c382a8f",
   "metadata": {},
   "source": [
    "Now the groupby reduction **looks OK** in terms of number of tasks but remember\n",
    "that rechunking to get to this point involves some communication overhead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e45f9-5b18-482a-8c76-66f81ff5710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "flox.xarray.xarray_reduce(rechunked, day, func=\"mean\", method=\"cohorts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c58969-5c99-4bc0-90ee-9cef468bf78b",
   "metadata": {},
   "source": [
    "## How about other climatologies?\n",
    "\n",
    "Let's try monthly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559ea33-5499-48ff-9a2e-5141c3a69fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "flox.xarray.xarray_reduce(oisst, oisst.time.dt.month, func=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00de8eb-e414-4920-8dcd-b64afbf91b62",
   "metadata": {},
   "source": [
    "This looks great. Why?\n",
    "\n",
    "It's because each chunk (size 20) is smaller than number of days in a typical\n",
    "month. `flox` initially applies the groupby-reduction blockwise. For the chunk\n",
    "size of 20, we will have at most 2 groups in each chunk, so the initial\n",
    "blockwise reduction is quite effective - at least a 10x reduction in size from\n",
    "20 elements in time to at most 2 elements in time.\n",
    "\n",
    "For this kind of problem, `\"map-reduce\"` works quite well.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

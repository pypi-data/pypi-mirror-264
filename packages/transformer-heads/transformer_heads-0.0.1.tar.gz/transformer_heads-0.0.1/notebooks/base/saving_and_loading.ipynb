{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading\n",
    "The models created with *transformer_heads* generally integrate well with huggingface and will work with automatic saving/checkpointing during training using for example the *Trainer* class. However, during loading it has to be ensured that all heads are attached correctly and that their parameters (and qlora parameters) are loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_heads import (\n",
    "    create_headed_qlora,\n",
    "    load_lora_with_heads,\n",
    "    HeadConfig,\n",
    "    load_headed,\n",
    "    get_multi_head_transformer,\n",
    ")\n",
    "from transformer_heads.util.helpers import get_model_params\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_path = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = get_model_params(model_path)\n",
    "model_class = model_params[\"model_class\"]\n",
    "hidden_size = model_params[\"hidden_size\"]\n",
    "vocab_size = model_params[\"vocab_size\"]\n",
    "print(model_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some random head configs for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = [\n",
    "    HeadConfig(\n",
    "        name=\"lm_head\",\n",
    "        layer_hook=-1,\n",
    "        in_size=hidden_size,\n",
    "        output_activation=\"linear\",\n",
    "        is_causal_lm=True,\n",
    "        loss_fct=\"cross_entropy\",\n",
    "        num_outputs=vocab_size,\n",
    "    ),\n",
    "    HeadConfig(\n",
    "        name=\"classification_hook\",\n",
    "        layer_hook=-4,\n",
    "        in_size=hidden_size,\n",
    "        hidden_size=1024,\n",
    "        num_layers=2,\n",
    "        output_activation=\"linear\",\n",
    "        is_causal_lm=False,\n",
    "        loss_fct=\"cross_entropy\",\n",
    "        num_outputs=2,\n",
    "    ),\n",
    "    HeadConfig(\n",
    "        name=\"regression_hook\",\n",
    "        layer_hook=-6,\n",
    "        in_size=4096,\n",
    "        output_activation=\"linear\",\n",
    "        is_causal_lm=False,\n",
    "        loss_fct=\"mse\",\n",
    "        num_outputs=1,\n",
    "        is_regression=True,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading a transformer with attached linear probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a quantized model with multiple heads\n",
    "model = load_headed(\n",
    "    model_class,\n",
    "    model_path,\n",
    "    heads,\n",
    "    device_map=\"cuda\",\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "# Now you would do some training ...\n",
    "# Save the model now\n",
    "model.save_pretrained(\"test_model\")\n",
    "# Model is saved, delete it\n",
    "del model\n",
    "\n",
    "# With load_headed we can load the quantized model with the heads\n",
    "model = load_headed(\n",
    "    model_class,\n",
    "    model_path,\n",
    "    head_folder_path=\"test_model\",\n",
    "    device_map=\"cuda\",\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading a model finetuned with qlora with extra heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some simple LoRA config. target_modules=None will result in all linear layers being adapted with LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    target_modules=None,\n",
    ")\n",
    "# create_headed_qlora is the way to go for models with LoRA and newly initialized heads\n",
    "model = create_headed_qlora(\n",
    "    base_model_class=model_class,\n",
    "    model_name=model_path,\n",
    "    quantization_config=quantization_config,\n",
    "    lora_config=lora_config,\n",
    "    head_configs=heads,\n",
    "    fully_trained_heads=True,\n",
    "    device_map={\"\": torch.cuda.current_device()},\n",
    ")\n",
    "# Now you would do some training ...\n",
    "# Saving is still easy using the huggingface api\n",
    "model.save_pretrained(\"test_model_qlora\")\n",
    "del model\n",
    "\n",
    "# Load the qlora model with it's heads. We only need the base model class and the save location. Loading quantized is fully optional here.\n",
    "model = load_lora_with_heads(\n",
    "    model_class,\n",
    "    \"test_model_qlora\",\n",
    "    quantization_config,\n",
    "    device_map={\"\": torch.cuda.current_device()},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sh_finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
